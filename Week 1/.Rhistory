fromJSON(raw.data)$daily_views
data.frame(fromJSON(raw.data)$daily_views)
data.frame(rownames(raw.datadata<-fromJSON(raw.data)
))
data<-fromJSON(raw.data)
class(data)
data[[1]]
library(jsonlite)
rd <- fromJSON(raw.data)
class(rd)
rd[[1]]
head(rd[[1]])
data.frame(rd[[1]])
library(RJSONIO)
rd <- fromJSON(raw.data)
class(rd)
head(rd[[1]])
data.frame(rd[[1]])
library(RJSONIO)
rd <- fromJSON(RJSONIO::raw.data)
library(RJSONIO)
rd <- JSONIO::fromJSON(raw.data)
class(rd)
head(rd[[1]])
data.frame(rd[[1]])
rd <- RJSONIO::fromJSON(raw.data)
class(rd)
head(rd[[1]])
help(ymd)
timeVector()
timeVector
ret <- seq(from = as.POSIXct(starttime), to = as.POSIXct(endtime),
by = timestep)
ret <- seq(from = as.POSIXct("2008-01-01"), to = as.POSIXct("2016-12-31"),
by = timestep)
ret <- seq(from = as.POSIXct("2008-01-01"), to = as.POSIXct("2016-12-31"),
by = "months")
ret
substr(ret, 0,6)
substr(ret, 0,7)
gsub("-","",substr(ret, 0,7))
raw.data
fromJSON(raw.data)
fromJSON(raw.data)$daily_views
unlist(fromJSON(raw.data)$daily_views)
data.frame(unlist(fromJSON(raw.data)$daily_views))
rd<-data.frame(rd)
rd$day <- rownames(rd)
rd
x=2011
eval(paste(x,"01",sep=""):paste(x,"12",sep=""))
datevector<- unlist(lapply(2008:2016, function(x)
paste(x,c("01","02","03","04","05","06","07","08","09","10","11","12"),sep="")))
head(datevector)
datevector <- seq(from = as.POSIXct("2008-01-01"), to = as.POSIXct("2016-12-31"),
by = "months")
datevector<-gsub("-","",substr(datevector, 0,7))
head(datevector)
datevector<- unlist(lapply(2008:2016, function(x) eval(paste(x,"01",sep=""):paste(x,"12",sep=""))))
head(datevector)
rd
plot(rd$day,rd$daily_views)
class(rd$daily_views)
class(rd$da)y
class(rd$day)
plot(as.POSIXct(rd$day),rd$daily_views)
plot(as.POSIXct(rd$day),rd$daily_views, type="l")
rd
rd[order(day)]
rd[order(rd$day)]
rd$day
order(rd$day)
rd[order(rd$day),]
rd<-rd[order(rd$day),]
plot(as.POSIXct(rd$day),rd$daily_views, type="l")
library(stringr)
str_replace()
help("str_replace")
str_split()
WORDS<-c("IBM", "Chicago")
WORDS[grep("^[A-Z]{+1}[a-z]*",WORDS)]
WORDS<-c("IBM", "Chicago")
WORDS[grep("^[A-Z]{+}[a-z]*",WORDS)]
WORDS<-c("IBM", "Chicago")
WORDS[grep("^[A-Z]{2,}[a-z]*",WORDS)]
WORDS<-c("IBM", "Chicago")
WORDS[-grep("^[A-Z]{2,}[a-z]*",WORDS)]
WORDS[-grep("[^A-Z]{2,}[a-z]*",WORDS)]
WORDS[grep("[^A-Z]{2,}[a-z]*",WORDS)]
WORDS[grep("[^A-Z]{2,}[a-z]*",WORDS)]
WORDS[grep("[^A-Z]{2,}[a-z]*",WORDS)]<-tolower(WORDS[grep("[^A-Z]{2,}[a-z]*",WORDS)])
WORDS
str_trim()
help("str_trim")
str_trim(" This is a test  .")
gsub("\\w{2,}","\\w"," This is a test  .")
gsub("\w{2,}","\w"," This is a test  .")
help(gsub)
gsub("[:blank:]{2,}","[:blank:]"," This is a test  .")
gsub("[:blank:]+","[:blank:]"," This is a test  .")
gsub("[:blank:]+","[:blank:]"," This is a test  .", perl=TRUE)
gsub("\w{2,}","\w"," This is a test  .")
gsub("\w{2,}","\\w"," This is a test  .")
gsub("]\w{2,}","\\w"," This is a test  .")
gsub("\\w{2,}","\\w"," This is a test  .")
gsub(" {2,}"," "," This is a test  .")
gsub(" {1,}"," "," This is a test  .")
gsub(" {1,}",""," This is a test  .")
gsub("[[:space:]]+", " ", A)
gsub("[[:space:]]+", " ", " This is a test  .")
gsub("[[:space:]]+", "[[:space:]]", " This is a test  .")
gsub("[[:space:]]+", "\[[:space:]]", " This is a test  .")
gsub("[[:space:]]+", " ", " This is a test  .")
gsub("[[:space:]]+", " ", " This is a  test.")
gsub("[[:space:]]+", " ", "  This is a  test.")
gsub("^[[:space:]]+", " ", "  This is a  test.")
gsub("[[:space:]]+", " ", "  This is a  test.")
gsub("[[:space:]]+", " ", "  This is a  test.")
gsub(" [[:space:]]+", " ", "  This is a  test.")
gsub("[[:space:]]+", " ", "  This is a  test.")
gsub("[[:space:]]+", "", "  This is a  test.")
gsub("[[:space:]]{2,}", "", "  This is a  test.")
substr(str, 5, nchar(states[1]))
states = rownames(USArrests)
tolower(states[0:4])
substr(str, 5, nchar(states[1]))
states]1
states[1]
help(substr)
substr(str, 5, 8)
substr(states[1], 5, nchar(states[1]))
substr(states[1], 3, nchar(states[1]))
var=201401
url=paste("http://stats.grok.se/json/en/",var,"/Donald_Trump",sep="")
raw.data <- readLines(url, warn="F")
raw.data
data<-fromJSON(raw.data)
class(data)
head(data[[1]])
library(RJSONIO)
rd <- RJSONIO::fromJSON(raw.data)
class(rd)
head(rd[[1]])
datevector<- unlist(lapply(2008:2016, function(x)
paste(x,c("01","02","03","04","05","06","07","08","09","10","11","12"),sep="")))
head(datevector)
datevector <- seq(from = as.POSIXct("2008-01-01"), to = as.POSIXct("2016-12-31"),
by = "months")
datevector<-gsub("-","",substr(datevector, 0,7))
head(datevector)
#this is not treated as character but as numeric as we create a sequence using the : operator
datevector<- unlist(lapply(2008:2016, function(x) eval(paste(x,"01",sep=""):paste(x,"12",sep=""))))
head(datevector)
rd<-unlist(lapply(datevector, function(x) fromJSON(readLines(
paste("http://stats.grok.se/json/en/",x,"/Donald_Trump",sep="") ))$daily_views))
rd
rd<-data.frame(rd)
rd
datevec
datevector
datevector<- unlist(lapply(2012:2016, function(x) eval(paste(x,"01",sep=""):paste(x,"12",sep=""))))
head(datevector)
rd<-unlist(lapply(datevector, function(x) fromJSON(readLines(
paste("http://stats.grok.se/json/en/",x,"/Donald_Trump",sep="") ))$daily_views))
rd
datevector<- unlist(lapply(2013:2016, function(x) eval(paste(x,"01",sep=""):paste(x,"12",sep=""))))
head(datevector)
rd<-unlist(lapply(datevector, function(x) fromJSON(readLines(
paste("http://stats.grok.se/json/en/",x,"/Donald_Trump",sep="") ))$daily_views))
rd
rd<-data.frame(rd)
rd
plot(rd)
plot(rd[,c(2,1)])
rd
cl
rd$date<-rownames(rd)
rd
class(rd$date)
rd$date<-as.POSIXct(rownames(rd))
rd$date<-strptime(rownames(rd), "%Y-%m-%d")
class(rd$date)
plot(rd)
plot(rd[, c(2,1)])
rd
datevector<- unlist(lapply(2014:2016, function(x) eval(paste(x,"01",sep=""):paste(x,"12",sep=""))))
head(datevector)
rd<-unlist(lapply(datevector, function(x) fromJSON(readLines(
paste("http://stats.grok.se/json/en/",x,"/Donald_Trump",sep="") ))$daily_views))
rd<-data.frame(rd)
rd$date<-strptime(rownames(rd), "%Y-%m-%d")
@
plot(rd)
plot(rd[, c(2,1)])
head(rd)
plot(rd$date, log(rd$rd+1))
rd<-rd[rd>0]
rd
rd<-unlist(lapply(datevector, function(x) fromJSON(readLines(
paste("http://stats.grok.se/json/en/",x,"/Donald_Trump",sep="") ))$daily_views))
rd<-data.frame(rd)
rd$date<-strptime(rownames(rd), "%Y-%m-%d")
rd<-rd[rd>0,]
rd
plot(rd$date, log(rd$rd+1))
plot(rd$date, rd$rd)
plot(rd$date, log(rd$rd+1), type="l")
rd<-rd[order(rd$date),]
plot(rd$date, log(rd$rd+1), type="l")
#download some wikipedia traffic statistics
var=201401
url=paste("http://stats.grok.se/json/en/",var,"/Donald_Trump",sep="")
raw.data <- readLines(url, warn="F")
raw.data
data<-fromJSON(raw.data)
class(data)
head(data[[1]])
url=paste("http://stats.grok.se/json/en/",var,"/Donald_Trump",sep="")
raw.data <- readLines(url, warn="F")
raw.data
library(RJSONIO)
data<-fromJSON(raw.data)
class(data)
head(data[[1]])
rm(ls())
rm(list=ls())
var=201401
url=paste("http://stats.grok.se/json/en/",var,"/Donald_Trump",sep="")
raw.data <- readLines(url, warn="F")
raw.data
library(RJSONIO)
data<-fromJSON(raw.data)
class(data)
head(data[[1]])
#download some wikipedia traffic statistics
var=201401
url=paste("http://stats.grok.se/json/en/",var,"/Donald_Trump",sep="")
raw.data <- readLines(url, warn="F")
raw.data
library(RJSONIO)
data<-fromJSON(raw.data)
data
nchar(states)
states[which(nchar(states))==5]
states = rownames(USArrests)
nchar(states)
states[which(nchar(states))==5]
nchar(states)
states[which(nchar(states))==5,]
states[which(nchar(states)==5)]
substring()
help("substring")
sentence="This is a sentence that is split by white spaces."
str_split(sentence,' ')
sentence="Two sentences example. The split occurs around."
str_split(sentence,'\\. [A-Z]*')
str_split(sentence,'\\. ')
str_split(sentence,'\\.')
sentence="Two sentences example. The split occurs around full-stops followed by white space."
str_split(sentence,'\\. ')
help("grep")
grep("([^<])*", "<a href='http://google.com'>This is an HTML link</a>"
)
gsub("([^<])*","\\1","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^<])*","\\2","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^<])*","","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^<])*","\\3","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^<])*","\\5","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^</])*","\\5","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^</])*","\\1","<a href='http://google.com'>This is an HTML link</a>")
gsub(">([^</])*","\\1","<a href='http://google.com'>This is an HTML link</a>")
gsub(">([^</])*","\\2","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^</])*","\\2","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^</])*","\2","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^</])*","","<a href='http://google.com'>This is an HTML link</a>")
gsub("([^</])*","$1","<a href='http://google.com'>This is an HTML link</a>")
txt2 <- "useRs may fly into JFK or laGuardia"
gsub("(\\w)(\\w*)(\\w)", "\\U\\1\\E\\2\\U\\3", txt2, perl=TRUE)
sub("(\\w)(\\w*)(\\w)", "\\U\\1\\E\\2\\U\\3", txt2, perl=TRUE)
grep("[a-z]", letters)
grep("\w", letters)
grep("\\w", letters)
help("grep")
str_extract
library(stringr)
str_extract
str_extract()
help(str_extract)
str_extract_all
shopping_list <- c("apples x4", "bag of flour", "bag of sugar", "milk x2")
str_extract(shopping_list, "\\d")
str_extract(shopping_list, "[a-z]+")
str_extract(shopping_list, "[a-z]{1,4}")
str_extract(shopping_list, "\\b[a-z]{1,4}\\b")
str_extract_all(shopping_list, "\\b[a-z]+\\b", simplify = TRUE)
strings <- c(" 219 733 8965", "329-293-8753 ", "banana", "595 794 7569",
"387 287 6718", "apple", "233.398.9187  ", "482 952 3315",
"239 923 8115 and 842 566 4692", "Work: 579-499-7527", "$1000",
"Home: 543.355.3679")
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
str_extract(strings, phone)
str_match(strings, phone)
strings
strings <- c(" 219 733 8965", "329-293-8753 ", "banana", "595 794 7569",
"387 287 6718", "apple", "233.398.9187  ", "482 952 3315",
"239 923 8115 and 842 566 4692", "Work: 579-499-7527", "$1000",
"Home: 543.355.3679")
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
str_extract(strings, phone)
str_match(strings, phone)
str_extract_all(strings, phone)
strings]
strings
str_match_all(strings, phone)
rbind(str_match_all(strings, phone))
rbindlist(str_match_all(strings, phone))
library(dplyr)
rbindlist(str_match_all(strings, phone))
library(data.table)
rbindlist(str_match_all(strings, phone))
strings <- c(" 219 733 8965", "329-293-8753 ", "banana", "595 794 7569",
"387 287 6718", "myweb@gmail.com", "233.398.9187  ", "482 952 3315",
"239 923 8115 and 842 566 4692", "Work: 579-499-7527", "Email: president@whitehouse.gov",
"Home: 543.355.3679")
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
strings <- c(" 219 733 8965", "329-293-8753 ", "banana", "595 794 7569",
"387 287 6718", "myweb@gmail.com", "233.398.9187  ", "482 952 3315",
"239 923 8115 and 842 566 4692", "Work: 579-499-7527", "Email: president@whitehouse.gov",
"Home: 543.355.3679")
##regex to match phone landlines
pattern <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
strings[grep(pattern,string)]
strings[grep(pattern,strings)]
str_extract(pattern,strings)
str_extract(strings,patterns)
str_extract(strings,pattern)
str_match(strings,pattern)
length(strings)
gsub(pattern,"\\1-\\2-\\3", strings)
gsub(pattern,"\\1-\\2-\\3", strings[grep(pattern,strings)])
gsub(pattern,"\\1-\\2-\\3", strings)
strings
HTML<-readLines(con="https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population")
#subsetting
HTML<-HTML[grep('<span class="mw-headline" id="States_and_territories">',HTML):grep('<span class="mw-headline" id="Summary_of_population_by_region">',HTML)]
head(HTML)
HTML.ROWS<-str_split(HTML, "<tr>")[[1]]
library(stringr)
HTML.ROWS<-str_split(HTML, "<tr>")[[1]]
HTML.ROWS
HTML.ROWS
HTML.ROWS<-str_split(HTML, "<tr>")
HTML.ROWS
HTML[grep("<tr>")]
HTML[grep("<tr>",HTML)]
grep("<tr>",HTML)
grep("</tr>",HTML)
HTML[14:24]
HTML[14:243
HTML[13:24]
length(
grep("<tr>",HTML)
)
length(
grep("</tr>",HTML)
)
grep("</tr>",HTML)
grep("<tr>",HTML)
lapply(grep("<tr>",HTML), function(x) gsub("<.*?>", "", HTML[x:(x+10)]))
lapply(grep("<tr>",HTML), function(x) gsub("<.*?>", "", HTML[x:(x+10)]))
unlist(lapply(grep("<tr>",HTML), function(x) gsub("<.*?>", "", HTML[x:(x+10)])))
TEMP<-lapply(grep("<tr>",HTML), function(x) gsub("<.*?>", "", HTML[x:(x+10)]))
TEMP<-lapply(TEMP, function(x) x[x!=""])
TEMP
do.call(TEMP, rbind)
do.call("rbind", TEMP)
TEMP<-do.call("rbind", TEMP)
class(TEMP)
library("rvest")
url <- "http://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population"
population <- url %>%
html() %>%
html_nodes(xpath='//*[@id="mw-content-text"]/table[1]') %>%
html_table()
install.packages("rvest")
library("rvest")
url <- "http://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population"
population <- url %>%
html() %>%
html_nodes(xpath='//*[@id="mw-content-text"]/table[1]') %>%
html_table()
population <- url %>%
read_html() %>%
html_nodes(xpath='//*[@id="mw-content-text"]/table[1]') %>%
html_table()
population
population <- population[[1]]
population
url <- 'http://www.huffingtonpost.com/2015/02/22/wisconsin-right-to-work_n_6731064.html'
api <- 'http://graph.facebook.com/comments?id='
target <- paste(api,url,sep="")
raw.data <- readLines(target, warn="F")
rd <- fromJSON(raw.data)
url <- 'https://www.huffingtonpost.com/2015/02/22/wisconsin-right-to-work_n_6731064.html'
api <- 'https://graph.facebook.com/comments?id='
target <- paste(api,url,sep="")
raw.data <- readLines(target, warn="F")
url <- 'http://www.huffingtonpost.com/2015/02/22/wisconsin-right-to-work_n_6731064.html'
api <- 'http://juicer.herokuapp.com/api/article?url='
target <- paste(api,url,sep="") target
raw.data <- readLines(target, warn="F")
url <- 'https://www.huffingtonpost.com/2015/02/22/wisconsin-right-to-work_n_6731064.html'
api <- 'https://juicer.herokuapp.com/api/article?url='
target <- paste(api,url,sep="") target
raw.data <- readLines(target, warn="F")
target <- paste(api,url,sep="")
raw.data <- readLines(target, warn="F")
target
raw.data
url <- 'http://www.huffingtonpost.com/entry/house-republicans-ethics_us_586bdb14e4b0de3a08f99e66?6ztihpvi'
api <- 'http://juicer.herokuapp.com/api/article?url='
target <- paste(api,url,sep="")
raw.data <- readLines(target, warn="F")
raw.data
rd <- fromJSON(raw.data)
library(RJSONIO)
rd <- fromJSON(raw.data)
rd
dat <- rd$article
dat$entities <-NULL
dat <-data.frame(dat)
dat
dat <- rd$article
dat
dat$entities
do.call("rbind", dat$entities)
load("../../Data/fb_oauth.rdata")
require("Rfacebook")
me <- getUsers("me", fb_oauth, private_info=TRUE)
me$name # my name
page <- getPage("barackobama", fb_oauth, n = 100)
strwrap(head(page$message))
grep("([^<])*", "<a href='http://google.com'>This is an HTML link</a>")
grep("([^<])*", c("<b>Bold text</b>", "Another text") )
grep("([^<])*", c("<b>Bold text</b>", "Another text") )
gsub("([[:alnum:]\\.]{3,})@([[:alnum:]]{3,})\\.([[a-z]]{2,})", "\\1 \\2 \\3", c("user123@gmail.com","tom.merit@hotmail.com")
gsub("([[:alnum:]\\.]{3,})@([[:alnum:]]{3,})\\.([[a-z]]{2,})", "\\1 \\2 \\3", c("user123@gmail.com","tom.merit@hotmail.com"))
gsub("([[:alnum:]]{3,})@([[:alnum:]]{3,})\\.([[a-z]]{2,})", "\\1 \\2 \\3", c("user123@gmail.com","tom.merit@hotmail.com"))
help(gsub)
gsub("([[:alnum:]]{3,})@([[:alnum:]]{3,})\\.([[a-z]]{2,})", "\\1", c("user123@gmail.com","tom.merit@hotmail.com"))
gsub("([[:alnum:]]{3,})@([[:alnum:]]{3,})\\.([[a-z]]{2,})", "\\2", c("user123@gmail.com","tom.merit@hotmail.com"))
gsub("([:alnum:]{3,})@([[:alnum:]]{3,})\\.([[a-z]]{2,})", "\\2", c("user123@gmail.com","tom.merit@hotmail.com"))
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.([[a-z]]{2,})", "\\2", c("user123@gmail.com","tom.merit@hotmail.com"))
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.([[a-z]]{2,})", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})\@([A-z0-9]{3,})\\.([[a-z]]{2,})", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})\\@([A-z0-9]{3,})\\.([[a-z]]{2,})", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})", "\\1", "test@devmag.net")
gsub("([A-z0-9]{3,})", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})@", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})@", "\\1", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})", "\\1", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})", "\\1", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.", "\\1", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.", "\\3", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.([A-z0-9]{3,})", "\\3", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.([A-z0-9]{3,})", "\\1", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.([A-z0-9]{3,})", "\\2", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.([A-z0-9]{3,})", "\\3", "test@devmag.net")
gsub("([A-z0-9]{3,})@([A-z0-9]{3,})\\.([A-z0-9]{3,})", "\\1 \\2 \\3", "test@devmag.net")
gsub("<a href=([^\\"]*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")"
gsub("<a href=([^\\"]*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("<a href=([^"]*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("<a href=([^\"]*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("<a href=([^\\"]*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("<a href=([^\\\"]*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("<a href=([]*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("<a href=(*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("<a href=(.*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("href=(.*)>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("href=(.*)\>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("href=(.*)\\>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("href=(.*)\\>([^<]*)", "\\1 \\2", '<a href="http://www.google.com">Link to Google</a>')
gsub("href=([^\\"]*)\\>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("href=([^\"]*)\\>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("href=([^\\\"]*)\\>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("href=([^\"]*)\\>([^<]*)", "\\1 \\2", "<a href="http://www.google.com">Link to Google</a>")
gsub("href=([^\"]*)\\>([^<]*)", "\\1 \\2", '<a href="http://www.google.com">Link to Google</a>')
gsub("href=([^\"]*)\\>([^<]*)", "\\2", '<a href="http://www.google.com">Link to Google</a>')
gsub("href=\"([^\"]*)\\>([^<]*)", "\\2", '<a href="http://www.google.com">Link to Google</a>')
gsub("href=\"([^\"]*)\\>([^<]*)", "\\1", '<a href="http://www.google.com">Link to Google</a>')
gsub("<a href=\"([^\"]*)\\>([^<]*)", "\\1", '<a href="http://www.google.com">Link to Google</a>')
gsub("<a href=\"([^\"]*)\\>([^<]*)</a>", "\\1", '<a href="http://www.google.com">Link to Google</a>')
gsub("<a href=\"([^\"]*)\\>([^<]*)</a>", "\\2", '<a href="http://www.google.com">Link to Google</a>')
gsub("<a href=\"([^\"]*)\"\>([^<]*)</a>", "\\2", '<a href="http://www.google.com">Link to Google</a>')
gsub("<a href=\"([^\"]*)\"\\>([^<]*)</a>", "\\2", '<a href="http://www.google.com">Link to Google</a>')
gsub("<a href=\"([^\"]*)\"\\>([^<]*)</a>", "\\1", '<a href="http://www.google.com">Link to Google</a>')
gsub("<a href=\"([^\"]*)\\>([^<]*)</a>", "\\1", '<a href="http://www.google.com">Link to Google</a>')
gsub("<a href=\"([^\"]*)\\>([^<]*)</a>", "\\2", '<a href="http://www.google.com">Link to Google</a>')
str_extract()
help("str_extract")
str_extract("http://doodle.com/poll/ga2thc6k5w9xa2z32kt452rz/", "poll/([:alnum:]*)/$")
str_extract_all("http://doodle.com/poll/ga2thc6k5w9xa2z32kt452rz/", "poll/([:alnum:]*)/$)
str_extract("http://doodle.com/poll/ga2thc6k5w9xa2z32kt452rz/", "poll/([:alnum:]*)/$")
str_match("http://doodle.com/poll/ga2thc6k5w9xa2z32kt452rz/", "poll/([:alnum:]*)/$")
str_match("http://doodle.com/poll/ga2thc6k5w9xa2z32kt452rz/", "poll/([:alnum:]*)/$")[,2]
gsub("<a href=\"([^\"]*)\\>([^<]*)</a>", "\\1", '<a href="http://www.google.com">Link to Google</a>')
str_match("http://doodle.com/poll/ga2thc6k5w9xa2z32kt452rz/", "poll/([:alnum:]*)/$")[,2]
library(stringr)
str_match("http://doodle.com/poll/ga2thc6k5w9xa2z32kt452rz/", "poll/([:alnum:]*)/$")[,2]
