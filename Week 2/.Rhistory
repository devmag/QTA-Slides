library(plyr)
library(reshape2)
library(plyr)
library(stringr)
library(quanteda)
library(data.table)
data(SOTUCorpus, package = "quantedaData")
TOKENS<-unlist(tokenize(subset(SOTUCorpus, Date>'1993-01-01'), ngrams=2, removeNumbers=TRUE, removePunct=TRUE, removeSymbols=TRUE, concatenator=" "))
DF<-data.table("token"=TOKENS, "president"=names(TOKENS))
DF[, .N, by=token][order(N, decreasing=TRUE)][1:15]
TOKENS<-removeFeatures(tokenize(subset(SOTUCorpus, Date>'1994-01-01'), removePunct = TRUE),stopwords("english"))
TOKENS<-unlist(ngrams(TOKENS, n=2, concatenator=" "))
DF<-data.table("token"=TOKENS, "president"=names(TOKENS))
DF<-DF[, .N, by=token][order(N, decreasing=TRUE)]
DF$N<-as.numeric(DF$N)
DF[1:10]
DF<-DF[1:500]
library(pacman)
pacman::p_load_gh(c(
"trinker/termco",
"trinker/tagger",
"trinker/textshape"
))
#THIS TAKES A WHILE
TAGGED<-unlist(lapply(lapply(tag_pos(DF$token), names), function(x) paste(x, collapse=" ")))
DF<-cbind(DF,TAGGED)
head(DF)
DF[, total:=sum(N)]
DF
DF[, w1 := str_extract(token,"^[A-z]*")]
DF[, w2 := str_extract(token,"[A-z]*$")]
DF
DF<-join(DF, DF[, list(c_1 = sum(N)), by=w1])
DF
DF<-join(DF, DF[, list(c_2 = sum(N)), by=w2])
DF
TOKENS<-removeFeatures(tokenize(subset(SOTUCorpus, Date>'1994-01-01'), removePunct = TRUE),stopwords("english"))
TOKENS<-unlist(ngrams(TOKENS, n=2, concatenator=" "))
DF<-data.table("token"=TOKENS, "president"=names(TOKENS))
DF<-DF[, .N, by=token][order(N, decreasing=TRUE)]
DF$N<-as.numeric(DF$N)
DF[1:10]
DF<-DF[1:500]
library(pacman)
pacman::p_load_gh(c(
"trinker/termco",
"trinker/tagger",
"trinker/textshape"
))
#THIS TAKES A WHILE
TAGGED<-unlist(lapply(lapply(tag_pos(DF$token), names), function(x) paste(x, collapse=" ")))
DF<-cbind(DF,TAGGED)
head(DF)
DF[, w1 := str_extract(token,"^[A-z]*")]
DF[, w2 := str_extract(token,"[A-z]*$")]
DF<-join(DF, DF[, list(c_1 = sum(N)), by=w1])
DF<-join(DF, DF[, list(c_2 = sum(N)), by=w2])
DF
DF[, sum(N)]
head(DF)
DF[,total := sum(total)]
DF[,total := sum(N)]
DF
DF[, ttest := (N/total - c1/N * c2/N)/(((N/total*(1-N/total))/total )^0.5) ]
DF[, ttest := (N/total - c_1/N * c_2/N)/(((N/total*(1-N/total))/total )^0.5) ]
DF
DF[, ttest := (N/total - c_1/total * c_2/total)/(((N/total*(1-N/total))/total )^0.5) ]
DF
DF[, ttest := (N/total - c_1/total * c_2/total)]
DF
DF[, denom := (((N/total*(1-N/total))/total )^0.5)]
DF
DF[, denom := (((N/total*(1-N/total))))]
DF
DF[, denom := (((N/total*(1-N/total)))/total)]
DF
DF[, ttest := (N/total - c_1/total * c_2/total)/(((N/total*(1-N/total))/total )^0.5) ]
DF
mu=sum(DF[grep("care$",token)]$N) *sum(DF[grep("^health",token)]$N)/(sum(DF$N)^2)
mu
(sum(DF[grep("care$",token)]$N)*sum(DF[grep("^health",token)]$N))/(sum(DF$N)^2)
DF[, ttest := (N/total - c_1/total * c_2/total)]
DF
DF[, denom := (((N/total*(1-N/total))/total )^0.5) ]
DF
DF[, denom := (((c_1/total * c_2/total*(1-c_1/total * c_2/total))/total )^0.5) ]
DF
DF[, ttest := (N/total - c_1/total * c_2/total)/(((c_1/total * c_2/total*(1-c_1/total * c_2/total))/total )^0.5)]
DF
DF
DF[, ttest := (N/total - c_1/total * c_2/total)]
DF
DF[, denom := (((c_1/total *c_2/total)*(1-c_1/total * c_2/total))/total )^0.5)]
DF[, denom := (((c_1/total *c_2/total)*(1-c_1/total * c_2/total))/total )^0.5))]
DF[, ttest := (N/total - c_1/total * c_2/total)/((c_1/total *c_2/total)*(1-c_1/total * c_2/total))/total )^0.5)]
DF[, ttest := (N/total - c_1/total * c_2/total)/(((c_1/total *c_2/total*(1-c_1/total * c_2/total))/total )^0.5)]
DF[, ttest := (N/total - c_1/total * c_2/total)/(((c_1/total *c_2/total*(1-c_1/total * c_2/total))/total )^0.5)]
DF
plot(hist(DF$ttest))
plot(density(DF$ttest))
plot(density(DF$ttest), main="Kernel Density of t-stats")
DF
DF[, ttest := NULL]
DF[, denom := NULL]
DF
DF[, temp:=N * (total-N- (c_1-N) - (c_2-N)]
DF[, temp:=N * (total-N- (c_1-N) - (c_2-N))]
DF[, temp:=(total-N- (c_1-N) - (c_2-N))]
DF
DF[, temp:=(c_2-N)]
DF
DF[, temp:=(c_1-N)]
DF
DF[, chi2 := (total * (N * (total-N- (c_1-N) - (c_2-N))) - (c_2-N)*(c_1-N))/((N +  (c_2-N))*(N+(c_1-N))*((c_2-N)*(total-N- (c_1-N) - (c_2-N)))*((c_1-N)*(total-N- (c_1-N) - (c_2-N))))]
DF
DF[, chi2 := (total * (N * (total-N- (c_1-N) - (c_2-N))) - (c_2-N)*(c_1-N))/((N +  (c_2-N))*(N+(c_1-N))*((c_2-N)+(total-N- (c_1-N) - (c_2-N)))*((c_1-N)+(total-N-(c_1-N) - (c_2-N))))]
DF
DF[, chi2 := total* ((N * (total-N- (c_1-N) - (c_2-N))) - (c_2-N)*(c_1-N))/((N +  (c_2-N))*(N+(c_1-N))*((c_2-N)+(total-N- (c_1-N) - (c_2-N)))*((c_1-N)+(total-N-(c_1-N) - (c_2-N))))]
DF
DF[,O11 := N]
DF[,O12 := (c_2-N)]
DF
DF[,O21 := (c_1-N)]
DF
DF[,O22 := (total- c_1  - c_2+N)]
DF
DF[, chi2 := total*((O11*O22 - O12*O21)^2)/( (O11+O12)*(O11+O21)*(O12+O22)*(O21+O22) )]
DF
DF[, chi2 := (total*(O11*O22 - O12*O21)^2)/( (O11+O12)*(O11+O21)*(O12+O22)*(O21+O22) )]
DF
DF[, num:= (total*(O11*O22 - O12*O21)^2)]
DF
DF[, denom := (O11+O12)*(O11+O21)*(O12+O22)*(O21+O22) ]
DF
DF[, num:= ((O11*O22 - O12*O21)^2)]
DF
((DF[token=="health care"]$N - sum(DF[grep("^\\bhealth\\b",token)]$N)/sum(DF$N) * sum(DF[grep("\\bcare\\b$",token)]$N)/sum(DF$N) * sum(DF$N))^2)/(sum(DF[grep("^\\bhealth\\b",token)]$N)/sum(DF$N) * sum(DF[grep("\\bcare\\b$",token)]$N)/sum(DF$N) * sum(DF$N))
DF[, chi2 := (total*(O11*O22 - O12*O21)^2)/( (O11+O12)*(O11+O21)*(O12+O22)*(O21+O22) )]
DF[order(chi2,decreasing=TRUE)]
DF[order(chi2,decreasing=TRUE)][1:20]
DF[order(chi2,decreasing=TRUE)][1:30]
DF[order(chi2,decreasing=TRUE)][40:50]
DF[c1!=c2]
DF[c_1!=c_2]
DF[order(chi2,decreasing=TRUE)][40:50]
DF[order(chi2,decreasing=TRUE)][1:10]
DF[order(chi2,decreasing=TRUE)][1:40]
DF[order(chi2,decreasing=TRUE)][1:100]
DF[order(chi2,decreasing=TRUE)][1:100]$chi2
DF
DF
plot
DF[, chi2 := ((O11*O22 - O12*O21)^2)/( (O11+O12)*(O11+O21)*(O12+O22)*(O21+O22) )]
DF
DF[, :=(O11, N)]
DF[, `:=`(O11, N)]
DF
DF[, expcell := c_1/total * c_2/total *total]
DF
DF[expcell<5]
DF[expcell>5]
library(knitr)
opts_chunk$set(fig.path='figures/knitr-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=110)
options(scipen = 1, digits = 3)
options(warn=-1)
setwd("~/Dropbox/Teaching/Quantitative text analysis/Week 2")
options(stringsAsFactors = FALSE)
library(ggplot2)
library(data.table)
library(RJSONIO)
library(quanteda)
library(data.table)
library(calibrate)
library(plyr)
library(reshape2)
library(plyr)
library(stringr)
library(quanteda)
library(data.table)
data(SOTUCorpus, package = "quantedaData")
TOKENS<-unlist(tokenize(subset(SOTUCorpus, Date>'1993-01-01'), ngrams=2, removeNumbers=TRUE, removePunct=TRUE, removeSymbols=TRUE, concatenator=" "))
DF<-data.table("token"=TOKENS, "president"=names(TOKENS))
DF[, .N, by=token][order(N, decreasing=TRUE)][1:15]
TOKENS<-removeFeatures(tokenize(subset(SOTUCorpus, Date>'1994-01-01'), removePunct = TRUE),stopwords("english"))
TOKENS<-unlist(ngrams(TOKENS, n=2, concatenator=" "))
DF<-data.table("token"=TOKENS, "president"=names(TOKENS))
DF<-DF[, .N, by=token][order(N, decreasing=TRUE)]
DF$N<-as.numeric(DF$N)
DF[1:10]
library(pacman)
pacman::p_load_gh(c(
"trinker/termco",
"trinker/tagger",
"trinker/textshape"
))
#THIS TAKES A WHILE
TAGGED<-unlist(lapply(lapply(tag_pos(DF$token), names), function(x) paste(x, collapse=" ")))
DF<-cbind(DF,TAGGED)
head(DF)
DF[,total := sum(N)]
DF[, w1 := str_extract(token,"^[A-z]*")]
DF[, w2 := str_extract(token,"[A-z]*$")]
DF<-join(DF, DF[, list(c_1 = sum(N)), by=w1])
DF<-join(DF, DF[, list(c_2 = sum(N)), by=w2])
cbind(head(DF[grep("^NN NN$",TAGGED)][,list(token,TAGGED)]),head(DF[grep("^JJ NN$",TAGGED)][,list(token,TAGGED)]))
DF[token=="health care"]$N
sum(DF[grep("^\\bhealth\\b",token)]$N)
sum(DF[grep("\\bcare\\b$",token)]$N)
sum(DF$N)
DF[, ttest := (N/total - c_1/total * c_2/total)/(((c_1/total *c_2/total*(1-c_1/total * c_2/total))/total )^0.5)]
DF[order(ttest, decreasing=TRUE)]
DF[order(ttest, decreasing=TRUE)]
[,c1+c2>10]
DF[order(ttest, decreasing=TRUE)][,c1+c2>10]
DF[order(ttest, decreasing=TRUE)][c_1+c_2>10]
DF[order(ttest, decreasing=TRUE)][c_1+c_2>10][1:20]
DF[order(ttest, decreasing=TRUE)][c_1+c_2>20][1:20]
DF[order(ttest, decreasing=TRUE)][c_1+c_2>20][1:15]
DF[order(ttest, decreasing=TRUE)][c_1+c_2>20][1:25]
DF[order(ttest, decreasing=TRUE)][c_1+c_2>10][1:25]
DF[order(ttest, decreasing=TRUE)][c_1+c_2>20][1:25]
DF[order(ttest, decreasing=TRUE)][c_1+c_2>20][1:20]
plot(density(DF$ttest), main="Kernel Density of t-stats")
plot(density(DF[c_1+c+2>20]$ttest), main="Kernel Density of t-stats")
plot(density(DF[c_1+c_2>20]$ttest), main="Kernel Density of t-stats")
plot(density(DF[c_1+c+2>20]$ttest), main="Kernel Density of t-stats")
DF[,O11 := N]
DF[,O12 := (c_2-N)]
DF[,O21 := (c_1-N)]
DF[,O22 := (total- c_1  - c_2+N)]
DF[, chi2 := (total*(O11*O22 - O12*O21)^2)/( (O11+O12)*(O11+O21)*(O12+O22)*(O21+O22) )]
DF[order(chi2, decreasing=TRUE)][1:20]
DF[, expcell := c_1/total * c_2/total *total]
DF
summary(DF$chi2)
DF[, chi2 := ((O11*O22 - O12*O21)^2)/( (O11+O12)*(O11+O21)*(O12+O22)*(O21+O22) )]
DF
DF[order(chi2,decreasing=TRUE)]
DF[order(chi2,decreasing=TRUE)][N>10]
DF[order(chi2,decreasing=TRUE)][N>10][1:20]
DF[order(chi2,decreasing=TRUE)][N>10][1:30]
DF[c_1+c_2>20][order(chi2, decreasing=TRUE)][1:30]
TOKENS
TOK<-data.table("sotu"=names(TOKENS),"token"=TOKENS)
TOK[, president := str_extract(sotu, "([A-z]*)")]
TOK<-TOK[, .N, by=c("president","token")][president %in% c("Bush","Obama")]
TOK<-join(TOK, DF[, list(token, TAGGED)])
TOK[order(N, decreasing=TRUE)][1:20]
WIDE<-join(TOK[president=="Bush"][, list(token, bushcount=as.numeric(N))], TOK[president=="Obama"][, list(token, obamacount=as.numeric(N))])
WIDE[is.na(obamacount)]$obamacount<-0
WIDE[is.na(bushcount)]$bushcount<-0
WIDE[, totalcount := obamacount+bushcount]
WIDE<-WIDE[order(totalcount, decreasing=TRUE)][totalcount>5]
WIDE[, totalobama := sum(obamacount)]
WIDE[, totalbush := sum(bushcount)]
WIDE[, chi2 := (totalobama+totalbush)*(bushcount*(totalobama-obamacount) - obamacount*(totalbush-bushcount))^2/((bushcount+obamacount)*(bushcount+(totalbush-bushcount))*(obamacount+(totalobama-obamacount))*((totalbush-bushcount)+(totalobama-obamacount)))]
WIDE
WIDE
length(TOKENS)
TOK<-data.table("sotu"=names(TOKENS),"token"=TOKENS)
TOK
TOK[, president := str_extract(sotu, "([A-z]*)")]
TOK<-TOK[, .N, by=c("president","token")][president %in% c("Bush","Obama")]
TOK
WIDE<-join(TOK[president=="Bush"][, list(token, bushcount=as.numeric(N))], TOK[president=="Obama"][, list(token, obamacount=as.numeric(N))])
WIDE
WIDE[is.na(bushcount)]
join(TOK[president=="Bush"][, list(token, bushcount=as.numeric(N))], TOK[president=="Obama"][, list(token, obamacount=as.numeric(N))], all.x=TRUE, all.y=TRUE)
help(join)
join(TOK[president=="Bush"][, list(token, bushcount=as.numeric(N))], TOK[president=="Obama"][, list(token, obamacount=as.numeric(N))], type="full")
WIDE
WIDE<-join(TOK[president=="Bush"][, list(token, bushcount=as.numeric(N))], TOK[president=="Obama"][, list(token, obamacount=as.numeric(N))], type="full")
WIDE
nrow(WIDE)
WIDE[is.na(obamacount)]$obamacount<-0
WIDE[is.na(bushcount)]$bushcount<-0
WIDE[, totalcount := obamacount+bushcount]
WIDE<-WIDE[order(totalcount, decreasing=TRUE)][totalcount>5]
WIDE<-data.table(join(TOK[president=="Bush"][, list(token, bushcount=as.numeric(N))], TOK[president=="Obama"][, list(token, obamacount=as.numeric(N))], type="full"))
WIDE[is.na(obamacount)]$obamacount<-0
WIDE[is.na(bushcount)]$bushcount<-0
WIDE[, totalcount := obamacount+bushcount]
WIDE<-WIDE[order(totalcount, decreasing=TRUE)][totalcount>5]
WIDE
WIDE<-data.table(join(TOK[president=="Bush"][, list(token, bushcount=as.numeric(N))], TOK[president=="Obama"][, list(token, obamacount=as.numeric(N))], type="full"))
WIDE[is.na(obamacount)]$obamacount<-0
WIDE[is.na(bushcount)]$bushcount<-0
WIDE[, totalcount := obamacount+bushcount]
WIDE
WIDE<-WIDE[order(totalcount, decreasing=TRUE)][totalcount>5]
WIDE
WIDE[, totalobama := sum(obamacount)]
WIDE[, totalbush := sum(bushcount)]
WIDE[, chi2 := (totalobama+totalbush)*(bushcount*(totalobama-obamacount) - obamacount*(totalbush-bushcount))^2/((bushcount+obamacount)*(bushcount+(totalbush-bushcount))*(obamacount+(totalobama-obamacount))*((totalbush-bushcount)+(totalobama-obamacount)))]
WIDE
WIDE[1:20][order(chi2, decreasing=TRUE)]
join(WIDE, DF)[grep("NN.? NN.?|JJ.? NN.?",TAGGED)]
join(WIDE, DF[, list(token, TAGGED)])[grep("NN.? NN.?|JJ.? NN.?",TAGGED)]
WIDE<-join(WIDE, DF[, list(token, TAGGED)])[grep("NN.? NN.?|JJ.? NN.?",TAGGED)]
WIDE[1:20][order(chi2, decreasing=TRUE)]
install.packages("rtimes")
library("rtimes")
options(nytimes_as_key = "654d24766e184a5995dbf9e3d43386a0")
res <- as_search(q="bailout", begin_date = "20081001", end_date = '20081201')
res
length(res)
res$meta
res <- as_search(q="clean energy", begin_date = "20081001", end_date = '20061201')
res
res <- as_search(q="clean energy", begin_date = "20081001", end_date = '20161201')
res
library(quanteda)
library(data.table)
data(SOTUCorpus, package = "quantedaData")
SOTUCorpus
summary(SOTUCorpus)
DF<-summary(SOTUCorpus)
DF
head(DF)
plot(DF$Types,DF$Tokens)
plot(DF[, list(Tokens,Types))]
plot(DF[, list(Tokens,Types)]
plot(DF[, list(Tokens,Types)])
plot(DF$Types,DF$Tokens
plot(DF$Types,DF$Tokens)
plot(log(DF$Types),log(DF$Tokens))
plot(log(DF$Types),log(DF$Tokens))+abline(lm(DF$Types ~ DF$Tokens))
plot(log(DF$Types),log(DF$Tokens))
abline(lm(DF$Types ~ DF$Tokens))
abline(lm(log(DF$Types) ~ log(DF$Tokens)))
plot(log(DF$Types),log(DF$Tokens))
abline(lm(log(DF$Types) ~ log(DF$Tokens)))
abline(lm(log(DF$Tokens) ~ log(DF$Types)))
summary(lm(log(DF$Tokens) ~ log                                             (DF$Types)))
library(quanteda)
library(data.table)
data(SOTUCorpus, package = "quantedaData")
DF<-summary(SOTUCorpus)
plot(log(DF$Types),log(DF$Tokens))+abline(lm(log(DF$Tokens) ~ log
(DF$Types)))
summary(lm(log(DF$Tokens) ~ log                                             (DF$Types)))
library(quanteda)
library(data.table)
data(SOTUCorpus, package = "quantedaData")
DF<-summary(SOTUCorpus)
plot(log(DF$Types),log(DF$Tokens))+abline(lm(log(DF$Tokens) ~ log
(DF$Types)))
summary(lm(log(DF$Tokens) ~ log                                             (DF$Types)))
DF<-summary(SOTUCorpus)
plot(log(DF$Types),log(DF$Tokens))+abline(lm(log(DF$Tokens) ~ log(DF$Types)))
data(SOTUCorpus, package = "quantedaData")
names(SOTUCorpus)
SOTUCorpus$metadata
SOTUCorpus$documents
names(SOTUCorpus$documents)
subset(inaugCorpus, President == "Adams")
subset(inaugCorpus, President == "Obama")
OBAMA<-SOTUCorpus[President=="Obama"]subset(inaugCorpus, President == "Obama")
OBAMA<-SOTUCorpus[President=="Obama"]
summary(SOTUCorpus)
OBAMA<-SOTUCorpus[Text=="Obama-2012"]
OBAMA<-subset(SOTUCorpus, Text=="Obama-2012")
summary(SOTUCorpus)
OBAMA<-subset(SOTUCorpus, President=="Obama")
OBAMA<-subset(SOTUCorpus, filename=="su1888.txt")
OBAMA<-subset(SOTUCorpus, filename=="su2012.txt")
summary(OBAMA)
TOK<-tokenize(OBAMA)
head(TOK)
TOK<-data.table(TOK)
TOK
TOK<-tokenize(OBAMA)
TOK<-data.table(token=unlist(TOK))
TOK[, .N, by=token]
TOK[, .N, by=token][order(N, decreasing=TRUE)]
TOK[, .N, by=token][order(N, decreasing=TRUE)][1:100]
help(tokenize)
OBAMA<-subset(SOTUCorpus, filename=="su2012.txt")
TOK<-tokenize(OBAMA, removePunct=TRUE)
TOK<-data.table(token=unlist(tolower(TOK)))
TOK
TOK<-tokenize(OBAMA, removePunct=TRUE)
TOK<-data.table(token=tolower(unlist(TOK)))
TOK
TOK[, .N, by=tokenize()]
TOK[, .N, by=token]
TOK[, .N, by=token][order(N, decreasing=TRUE)]
TOK[, .N, by=token][order(N, decreasing=TRUE)]
OBAMA<-subset(SOTUCorpus, filename=="su2012.txt")
TOK<-tokenize(OBAMA, removePunct=TRUE)
TOK<-data.table(token=tolower(unlist(TOK)))
TOK<-TOK[, .N, by=token][order(N, decreasing=TRUE)]
TOK[1:20]
TOK[,rank :=1:nrow(TOK)]
TOK
tokenize()
TOK1<-tokenize(OBAMA, what="fastestword")
TOK2<-tokenize(OBAMA, what="fasterword")
TOK3<-tokenize(OBAMA, what="fasterword")
unlist(TOK1)
unlist(TOK1)
TOK1<-data.table(tok1=unlist(TOK1))
i="fastestword"
TOK<-data.table(tok=unlist(tokenize(OBAMA, what=i, removePunct=TRUE)))[ ,.N, by=tok]
TOK
for(i in c("word", "fastestword", "fasterword")) {
TOK<-data.table(tok=unlist(tokenize(OBAMA, what=i, removePunct=TRUE)))[ ,.N, by=tok]
TOK[, rid := 1:nrow(TOK)]
plot(log(TOK$N),log(TOK$rid))
}
TOK
TOK
TOK<-data.table(tok=unlist(tokenize(OBAMA, what=i, removePunct=TRUE)))[ ,.N, by=tok][order(N, decreasing=TRUE)]
TOK[, rid := 1:nrow(TOK)]
plot(log(TOK$N),log(TOK$rid))
summary(lm(log(TOK$N)~log(TOK$rid)
summary(lm(log(TOK$N)~log(TOK$rid))
)
summary(lm(log(TOK$N)~log(TOK$rid)))
summary(lm(log(TOK$N)~log(TOK$rid)))$coefficients
summary(lm(log(TOK$N)~log(TOK$rid)))$coefficients[1]
summary(lm(log(TOK$N)~log(TOK$rid)))$coefficients[2]
for(i in c("word", "fastestword", "fasterword")) {
TOK<-data.table(tok=unlist(tokenize(OBAMA, what=i, removePunct=TRUE)))[ ,.N, by=tok][order(N, decreasing=TRUE)]
TOK[, rid := 1:nrow(TOK)]
plot(log(TOK$N),log(TOK$rid), main=paste("b =",summary(lm(log(TOK$N)~log(TOK$rid)))$coefficients[2]))
}
plot(density(log(TOK$N)))
TOK
hist()
plot(hist(log(TOK$N)))
plot(hist(TOK$N))
plot(hist(log(TOK$N)))
plot(hist(log(TOK$N))) + lines(density(log(TOK$N)))
plot(hist(log(TOK$N)), main=i)
install.packages("hunspell")
TOK
TOK[, stemmed := wordStem("tok")]
library(SnowballC)
TOK[, stemmed := wordStem("tok")]
TOK
TOK[, stemmed := wordStem(tok)]
TOK
TOK[, sum(N),by=stemmed]
STEM<-TOK[, sum(N),by=stemmed]
plot(hist(STEM$V1))
plot(density(STEM$V1))
plot(density(STEM$V1)) + line(density(TOK$N))
plot(density(STEM$V1)) + lines(density(TOK$N))
plot(density(log(STEM$V1)))
plot(density(log(STEM$V1))) + lines(density(log(TOK$N)))
summary(TOK$N)
summary(STEM$V1)
plot(hist(log(STEM$V1)))
library("SnowballC")
wordStem("Amazing")
#multiple languages are supported
getStemLanguages()
wordStem("Liebschaften", language="de")
wordStem("amaren", language="es")
#densification?
TOK[, stemmed := wordStem("tok")]
nrow(TOK)
summary(TOK$N)
STEM<-TOK[, sum(N),by=stemmed]
plot(hist(log(STEM$V1)))
STEM
words <- c("love", "loving", "lovingly", "loved", "lover", "lovely", "love")
hunspell_stem(words)
hunspell_analyze(words)
library(hunspell)
words <- c("love", "loving", "lovingly", "loved", "lover", "lovely", "love")
hunspell_stem(words)
hunspell_analyze(words)
words <- c("police", "policy", "policing")
hunspell_stem(words)
hunspell_analyze(words)
words <- c("police", "policy", "policing")
hunspell_stem(words)
hunspell_analyze(words)
wordStem(words)
words <- c("police", "policy", "policies", policing")
hunspell_stem(words)
hunspell_analyze(words)
words <- c("police", "policy", "policies", policing")
hunspell_stem(words)
hunspell_analyze(words)
words <- c("police", "policy", "policies", "policing")
hunspell_stem(words)
hunspell_analyze(words)
wordStem(words)
words <- c("Severing", "several", "ironic", "iron", "animal", "animated")
wordStem(words)
hunspell_stem(words)
hunspell_stem("am")
hunspell_stem("I am")
levenshteinSim("MIN OF EDUCATION	PUPAL AL I AHMED	I",c("CHIEF OF STATE	ZAHIR SHAH, MUHAMMAD KING	H","PRIME MINISTER	DAUD, MUHAMMAD SARDAR	H","FIRST DEPUTY PRIME MINISTER	MUHAMMAD» ALI SARDAR	H","SECOND DEPUTY PRIME MINISTER	NA IM».MUHAMMAD SARDAR	H","MIN OF AGRICULTURE	ADALAT » GHOLAM HAIDER	H","MIN OF COMMERCE	'	. SHERZAD♦ GHOLAM MUHAMMAD	H","^	MIN OF COMMUNICATIONS	MORID, MUHAMMAD	H","MIN OF COURT	.	ALI, SULEIMAN AHMAD SARDAR	■","MIN OF DEFENSE	DAUD, MUHAMMAD SARDAR	H","MIN OF EDUCATION	POPAL, ALI AHMAD	H","^ , MIN OF FINANCE	MALIKYAR, ABDULLAH	H","MIN OF FOREIGN AFFAIRS	NAIM, MUHAMMAD SARDAR	■","MIN OF INTERIOR	ABDULLAH, SAYYED	■","MIN OF JUSTICE	ABDULLAH, SAYYED	H","MIN OF MINES & INDUSTRIES	YUSEF, MIR MUHAMMAD DR	:	■","^	MIN OF PLANNING	DAUD, MUHAMMAD SARDAR	I","MIN	OF	PUBLIC	HEALTH	■","MIN	OF	PUBLIC	WORKS	H","DEPT OF PRESS	SOHEIL, MUHAMMAD ASEF	H","DEPT OF TRIBAL AFFAIRS	MAJRUH, SAYYED SHAMSUDDIN	H"))
SIM<-levenshteinSim("MIN OF EDUCATION	PUPAL AL I AHMED	I",c("CHIEF OF STATE	ZAHIR SHAH, MUHAMMAD KING	H","PRIME MINISTER	DAUD, MUHAMMAD SARDAR	H","FIRST DEPUTY PRIME MINISTER	MUHAMMAD» ALI SARDAR	H","SECOND DEPUTY PRIME MINISTER	NA IM».MUHAMMAD SARDAR	H","MIN OF AGRICULTURE	ADALAT » GHOLAM HAIDER	H","MIN OF COMMERCE	'	. SHERZAD♦ GHOLAM MUHAMMAD	H","^	MIN OF COMMUNICATIONS	MORID, MUHAMMAD	H","MIN OF COURT	.	ALI, SULEIMAN AHMAD SARDAR	■","MIN OF DEFENSE	DAUD, MUHAMMAD SARDAR	H","MIN OF EDUCATION	POPAL, ALI AHMAD	H","^ , MIN OF FINANCE	MALIKYAR, ABDULLAH	H","MIN OF FOREIGN AFFAIRS	NAIM, MUHAMMAD SARDAR	■","MIN OF INTERIOR	ABDULLAH, SAYYED	■","MIN OF JUSTICE	ABDULLAH, SAYYED	H","MIN OF MINES & INDUSTRIES	YUSEF, MIR MUHAMMAD DR	:	■","^	MIN OF PLANNING	DAUD, MUHAMMAD SARDAR	I","MIN	OF	PUBLIC	HEALTH	■","MIN	OF	PUBLIC	WORKS	H","DEPT OF PRESS	SOHEIL, MUHAMMAD ASEF	H","DEPT OF TRIBAL AFFAIRS	MAJRUH, SAYYED SHAMSUDDIN	H"))
SIM
which.max(SIM)
##run on whole vector
VEC<-c("CHIEF OF STATE	ZAHIR SHAH, MUHAMMAD KING	H","PRIME MINISTER	DAUD, MUHAMMAD SARDAR	H","FIRST DEPUTY PRIME MINISTER	MUHAMMAD» ALI SARDAR	H","SECOND DEPUTY PRIME MINISTER	NA IM».MUHAMMAD SARDAR	H","MIN OF AGRICULTURE	ADALAT » GHOLAM HAIDER	H","MIN OF COMMERCE	'	. SHERZAD♦ GHOLAM MUHAMMAD	H","^	MIN OF COMMUNICATIONS	MORID, MUHAMMAD	H","MIN OF COURT	.	ALI, SULEIMAN AHMAD SARDAR	■","MIN OF DEFENSE	DAUD, MUHAMMAD SARDAR	H","MIN OF EDUCATION	POPAL, ALI AHMAD	H","^ , MIN OF FINANCE	MALIKYAR, ABDULLAH	H","MIN OF FOREIGN AFFAIRS	NAIM, MUHAMMAD SARDAR	■","MIN OF INTERIOR	ABDULLAH, SAYYED	■","MIN OF JUSTICE	ABDULLAH, SAYYED	H","MIN OF MINES & INDUSTRIES	YUSEF, MIR MUHAMMAD DR	:	■","^	MIN OF PLANNING	DAUD, MUHAMMAD SARDAR	I","MIN	OF	PUBLIC	HEALTH	■","MIN	OF	PUBLIC	WORKS	H","DEPT OF PRESS	SOHEIL, MUHAMMAD ASEF	H","DEPT OF TRIBAL AFFAIRS	MAJRUH, SAYYED SHAMSUDDIN	H")
SIM<-levenshteinSim("MIN OF EDUCATION	PUPAL AL I AHMED	I",VEC)
SIM
VEC[which.max(SIM)]
library(pacman)
pacman::p_load_gh(c(
"trinker/termco",
"trinker/tagger",
"trinker/textshape"
))
